# 缓存淘汰策略之LFU算法分析

LFU全称最不经常使用算法(Least Frequently Used)，LFU算法的基本思想和所有的缓存算法一样，都是基于locality假设(局部性原理)。

> 如果一个信息正在被访问，那么在近期它很可能还会被再次访问。

LFU是基于这种思想进行设计：一定时期内被访问次数最少的页，在将来被访问的机率也是最小的。

相比于LRU(Least Recenty Use)算法，LFU更加注重于使用效率。

## 原理

LFU将数据和数据的访问频次保存在一个容量有限的容器中，当访问一个数据时：

1. 该数据在容器中，则将该数据的访问频次加1。

2. 该数据不在容器中，则将该数据加入到容器中，且访问频次为1。

当数据量达到容器的限制后，会剔除掉访问频次最低的数据，下图是一个简易的LFU算法示意图

![](../assets/b24c750436a508c67e8287da8c887c97_1.png)

上图中的LRU容器是一个链表，会动态地根据访问频次调整数据在链表中的位置，方便进行数据的淘汰，可以看到，在第四步时，因为需要插入数据F，而淘汰了数据E。

## LFU算法的实现

![](../assets/b24c750436a508c67e8287da8c887c97_2.png)

```go
package LFU

type dNode struct {
    Key         int
    Val         int
    Freq        int
    Prev,Next   *dNode  
}

type dList struct {
    head, tail *node
}
 
func newDoubleList() *dList {
    return &dLinkedList{}
}
 
func (d *dLinkedList) addFirst(node *dNode) {
    tmpNode := d.head.Next
    node.Prev = d.head
    node.Next = tmpNode
    tmpNode.Prev = node
    d.head.Next = node
}
  
func (d *dLinkedList) remove(node *dNode) {
    node.Prev.Next = node.Next
    node.Next.Prev = node.Prev
    node.Next, node.Prev = nil, nil
}
 
type LFU struct {
    length      int
    capacity    int
    keyCache    map[int]*node
    freqCache   map[int]*dLinkedList
    minCache    *node
}
 
func Constructor(capacity int) LFUCache {
    return &LFU{
        length:    0,
        capacity:  capacity,
        keyCache:  map[int]*node{},
        freqCache: map[int]*dLinkedList{},
        minCache:  nil,
    }
}
 
func (l *LFU) Get(key int) int {
    x, ok := this.keyCache[key]
    if ok {
        f := x.freq
        // 移动出当前
        this.freqCache[x.freq].remove(x)
        x.freq++
        // 加到上一个频率第一个
        if this.freqCache[x.freq] == nil {
            this.freqCache[x.freq] = newDLinkedList()
        }
        this.freqCache[x.freq].addFirst(x)
        // 看情况更新minCache
        if this.minCache == x {
            // 当前还有没有
            if this.freqCache[f].head.post == this.freqCache[f].tail {
                this.minCache = this.freqCache[f+1].tail.pre
            } else {
                this.minCache = this.freqCache[f].tail.pre
            }
        }
        return x.val
    }
    return -1
}
 
func (this *LFUCache) Put(key int, value int) {
    if this.capacity == 0 {
        return
    }
    x, ok := this.keyCache[key]
    if ok {
        x.val = value
        f := x.freq
        // 移动出当前
        this.freqCache[x.freq].remove(x)
        x.freq++
        // 加到上一个频率第一个
        if this.freqCache[x.freq] == nil {
            this.freqCache[x.freq] = newDLinkedList()
        }
        this.freqCache[x.freq].addFirst(x)
        // 看情况更新minCache
        if this.minCache == x {
            // 当前还有没有
            if this.freqCache[f].head.post == this.freqCache[f].tail {
                this.minCache = this.freqCache[f+1].tail.pre
            } else {
                this.minCache = this.freqCache[f].tail.pre
            }
        }
    } else {
        // 不在，新建
        newNode := &node{
            key:  key,
            val:  value,
            freq: 1,
            pre:  nil,
            post: nil,
        }
        if this.size == this.capacity {
            f := this.minCache.freq
            this.freqCache[f].remove(this.minCache)
            delete(this.keyCache, this.minCache.key)
            this.minCache = nil
        } else {
            this.size++
        }
        this.keyCache[key] = newNode
        if this.freqCache[newNode.freq] == nil {
            this.freqCache[newNode.freq] = newDLinkedList()
        }
        this.freqCache[newNode.freq].addFirst(newNode)
        this.minCache = this.freqCache[newNode.freq].tail.pre
    }
}

```
## LFU和LRU对比

区别：LFU是基于访问频次的模式，而LRU是基于访问时间的模式。

LRU：当缓存的访问是循环的顺序遍历，而且缓存容量不足以容纳遍历的项时，缓存项就会频繁进出影响效率。

LFU：克服了 LRU 在大规模遍历时的缺点。但是容易导致旧数据的积累。同时新数据因为使用次数少容易反复被移出缓存导致长期无法积累跟大的使用次数。

优势：在数据访问符合正态分布时，相比LRU算法，LFU算法的缓存命中率会更高一些。

劣势：

1. LFU的复杂度要比LRU更高一些。

2. 需要维护数据的访问频次，每次访问都需要更新。

3. 早期的数据相比于后期的数据更容易被缓存下来，导致后期的数据很难被缓存。

4. 新加入缓存的数据很容易被剔除，像是缓存的末端发生“抖动”。

## LFU算法的优化方向

1. 更加紧凑的数据结构，避免维护访问频次的高消耗。

2. 避免早期的热点数据一直占据缓存，即LFU算法也需有一些访问时间模式的特性。

3. 消除缓存末端的抖动。

